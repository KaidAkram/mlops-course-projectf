{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Symptom Association Discovery\n",
    "## Unsupervised Learning with Apriori Algorithm\n",
    "\n",
    "**Author**: Your Name  \n",
    "**Date**: 2024  \n",
    "**Objective**: Discover symptom co-occurrence patterns and disease associations using Association Rule Mining\n",
    "\n",
    "---\n",
    "\n",
    "### Project Overview\n",
    "- **Technique**: Apriori Algorithm (Association Rule Mining)\n",
    "- **Application**: Medical symptom pattern discovery\n",
    "- **Output**: Association rules, visualizations, mobile app model\n",
    "\n",
    "### Key Metrics\n",
    "- **Support**: Frequency of symptom combinations\n",
    "- **Confidence**: Probability of consequent given antecedent\n",
    "- **Lift**: Strength of association (>1 = positive correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running on Kaggle)\n",
    "# !pip install mlxtend networkx plotly kaleido -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation\n",
    "\n",
    "We'll generate synthetic medical data with realistic symptom-disease patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data_generator.py code here or upload dataset\n",
    "# For Kaggle, you can upload a CSV or use the data generator\n",
    "\n",
    "# Example: Load from Kaggle dataset\n",
    "# df = pd.read_csv('/kaggle/input/disease-symptom-prediction/dataset.csv')\n",
    "\n",
    "# Or generate synthetic data (paste data_generator code here)\n",
    "# For brevity, assuming data is loaded\n",
    "\n",
    "print(\"Data loaded successfully\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract symptom columns\n",
    "symptom_cols = [col for col in df.columns \n",
    "               if col not in ['patient_id', 'disease', 'num_symptoms', 'symptoms']]\n",
    "\n",
    "print(f\"Total symptoms: {len(symptom_cols)}\")\n",
    "print(f\"\\nSample symptoms: {symptom_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary matrix for Apriori\n",
    "df_binary = df[symptom_cols].copy()\n",
    "\n",
    "print(f\"Binary matrix shape: {df_binary.shape}\")\n",
    "print(f\"\\nSample:\")\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symptom frequency\n",
    "symptom_freq = df_binary.sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "symptom_freq.head(20).plot(kind='bar', color='steelblue', alpha=0.8)\n",
    "plt.title('Top 20 Most Common Symptoms', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Symptom', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMost common symptom: {symptom_freq.index[0]} ({symptom_freq.iloc[0]} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disease distribution\n",
    "if 'disease' in df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df['disease'].value_counts().plot(kind='bar', color='coral', alpha=0.8)\n",
    "    plt.title('Disease Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Disease', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Association Rule Mining with Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set thresholds\n",
    "MIN_SUPPORT = 0.05\n",
    "MIN_CONFIDENCE = 0.6\n",
    "MIN_LIFT = 1.2\n",
    "\n",
    "print(f\"Mining parameters:\")\n",
    "print(f\"  Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"  Min Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"  Min Lift: {MIN_LIFT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Apriori algorithm\n",
    "print(\"Mining frequent itemsets...\")\n",
    "frequent_itemsets = apriori(df_binary, min_support=MIN_SUPPORT, use_colnames=True)\n",
    "\n",
    "print(f\"\\n✓ Found {len(frequent_itemsets)} frequent itemsets\")\n",
    "frequent_itemsets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules\n",
    "print(\"Generating association rules...\")\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=MIN_CONFIDENCE)\n",
    "\n",
    "# Filter by lift\n",
    "rules = rules[rules['lift'] >= MIN_LIFT]\n",
    "rules = rules.sort_values('lift', ascending=False)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(rules)} association rules\")\n",
    "rules.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rule Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top rules in readable format\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 ASSOCIATION RULES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in rules.head(10).iterrows():\n",
    "    antecedents = ', '.join(list(row['antecedents']))\n",
    "    consequents = ', '.join(list(row['consequents']))\n",
    "    \n",
    "    print(f\"\\nRule {idx + 1}:\")\n",
    "    print(f\"  IF: {antecedents}\")\n",
    "    print(f\"  THEN: {consequents}\")\n",
    "    print(f\"  Support: {row['support']:.3f} | Confidence: {row['confidence']:.3f} | Lift: {row['lift']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vs Confidence Scatter Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "scatter = plt.scatter(rules['support'], rules['confidence'], \n",
    "                     c=rules['lift'], s=rules['lift']*50, \n",
    "                     alpha=0.6, cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel('Support', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Confidence', fontsize=12, fontweight='bold')\n",
    "plt.title('Association Rules: Support vs Confidence (sized by Lift)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Lift')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Rules Bar Chart\n",
    "top_rules = rules.nlargest(20, 'lift').copy()\n",
    "top_rules['rule'] = top_rules.apply(\n",
    "    lambda row: f\"{', '.join(list(row['antecedents']))} → {', '.join(list(row['consequents']))}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "y_pos = np.arange(len(top_rules))\n",
    "plt.barh(y_pos, top_rules['lift'], color='steelblue', alpha=0.8)\n",
    "plt.yticks(y_pos, top_rules['rule'], fontsize=9)\n",
    "plt.xlabel('Lift', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 20 Association Rules by Lift', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symptom Co-occurrence Heatmap\n",
    "co_occurrence = df_binary.T.dot(df_binary)\n",
    "top_symptoms = symptom_freq.head(20).index\n",
    "co_occurrence_top = co_occurrence.loc[top_symptoms, top_symptoms]\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(co_occurrence_top, annot=True, fmt='d', cmap='YlOrRd', \n",
    "           square=True, linewidths=0.5, cbar_kws={'label': 'Co-occurrence Count'})\n",
    "plt.title('Top 20 Symptom Co-occurrence Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for _, row in rules.nlargest(30, 'lift').iterrows():\n",
    "    for ant in list(row['antecedents']):\n",
    "        for cons in list(row['consequents']):\n",
    "            if G.has_edge(ant, cons):\n",
    "                G[ant][cons]['weight'] += row['lift']\n",
    "            else:\n",
    "                G.add_edge(ant, cons, weight=row['lift'])\n",
    "\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "node_sizes = [G.degree(node) * 300 for node in G.nodes()]\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
    "                      node_color='lightblue', alpha=0.9, \n",
    "                      edgecolors='darkblue', linewidths=2)\n",
    "\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "nx.draw_networkx_edges(G, pos, width=[w/max(weights)*5 for w in weights],\n",
    "                      alpha=0.5, edge_color='gray', \n",
    "                      arrows=True, arrowsize=20)\n",
    "\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "plt.title('Symptom Association Network (Top 30 Rules)', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Algorithm Performance Comparison\n",
    "\n",
    "We will compare three algorithms: **Apriori**, **FP-Growth**, and **ECLAT** (custom implementation) in terms of execution time and memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ECLAT Implementation\n",
    "import time\n",
    "\n",
    "class ECLAT:\n",
    "    def __init__(self, min_support=0.05, min_items=1):\n",
    "        self.min_support = min_support\n",
    "        self.min_items = min_items\n",
    "        self.item_tid_sets = {}\n",
    "        self.frequent_itemsets = []\n",
    "        self.start_time = 0\n",
    "        self.end_time = 0\n",
    "\n",
    "    def fit(self, df_binary):\n",
    "        self.start_time = time.time()\n",
    "        self.n_transactions = len(df_binary)\n",
    "        self.min_support_count = self.min_support * self.n_transactions\n",
    "        \n",
    "        # 1. Transform horizontal to vertical format (Item -> TID set)\n",
    "        for col in df_binary.columns:\n",
    "            tids = set(df_binary.index[df_binary[col] == 1].tolist())\n",
    "            if len(tids) >= self.min_support_count:\n",
    "                self.item_tid_sets[frozenset([col])] = tids\n",
    "                \n",
    "        # 2. Mine recursively\n",
    "        self._mine(list(self.item_tid_sets.keys()))\n",
    "        \n",
    "        self.end_time = time.time()\n",
    "        return self\n",
    "\n",
    "    def _mine(self, itemsets):\n",
    "        for i in range(len(itemsets)):\n",
    "            itemset_i = itemsets[i]\n",
    "            tids_i = self.item_tid_sets[itemset_i]\n",
    "            self.frequent_itemsets.append((itemset_i, len(tids_i)/self.n_transactions))\n",
    "            \n",
    "            suffix_itemsets = []\n",
    "            for j in range(i + 1, len(itemsets)):\n",
    "                itemset_j = itemsets[j]\n",
    "                tids_j = self.item_tid_sets[itemset_j]\n",
    "                tids_join = tids_i.intersection(tids_j)\n",
    "                \n",
    "                if len(tids_join) >= self.min_support_count:\n",
    "                    new_itemset = itemset_i.union(itemset_j)\n",
    "                    self.item_tid_sets[new_itemset] = tids_join\n",
    "                    suffix_itemsets.append(new_itemset)\n",
    "            \n",
    "            if suffix_itemsets:\n",
    "                self._mine(suffix_itemsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "def run_apriori(df, min_support):\n",
    "    start = time.time()\n",
    "    res = apriori(df, min_support=min_support, use_colnames=True)\n",
    "    end = time.time()\n",
    "    return end - start, len(res)\n",
    "\n",
    "def run_fpgrowth(df, min_support):\n",
    "    start = time.time()\n",
    "    res = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
    "    end = time.time()\n",
    "    return end - start, len(res)\n",
    "\n",
    "def run_eclat(df, min_support):\n",
    "    model = ECLAT(min_support=min_support)\n",
    "    model.fit(df)\n",
    "    return model.end_time - model.start_time, len(model.frequent_itemsets)\n",
    "\n",
    "def measure_performance(func, *args):\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    result = func(*args)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    return end_time - start_time, peak / (1024 * 1024), result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Comparison\n",
    "supports = [0.2, 0.1, 0.05, 0.03]\n",
    "results = {'Support': supports, 'Apriori': [], 'FP-Growth': [], 'ECLAT': []}\n",
    "times = {'Apriori': [], 'FP-Growth': [], 'ECLAT': []}\n",
    "\n",
    "print(f\"{'Support':<8} | {'Apriori (s)':<12} | {'FP-Growth (s)':<12} | {'ECLAT (s)':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for sup in supports:\n",
    "    # Measure Apriori\n",
    "    t_ap, m_ap, _ = measure_performance(lambda: run_apriori(df_binary, sup))\n",
    "    times['Apriori'].append(t_ap)\n",
    "    \n",
    "    # Measure FP-Growth\n",
    "    t_fp, m_fp, _ = measure_performance(lambda: run_fpgrowth(df_binary, sup))\n",
    "    times['FP-Growth'].append(t_fp)\n",
    "    \n",
    "    # Measure ECLAT\n",
    "    t_ec, m_ec, _ = measure_performance(lambda: run_eclat(df_binary, sup))\n",
    "    times['ECLAT'].append(t_ec)\n",
    "    \n",
    "    print(f\"{sup:<8.2f} | {t_ap:.4f}s      | {t_fp:.4f}s      | {t_ec:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Execution Time Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(supports, times['Apriori'], marker='o', label='Apriori', linewidth=2)\n",
    "plt.plot(supports, times['FP-Growth'], marker='s', label='FP-Growth', linewidth=2)\n",
    "plt.plot(supports, times['ECLAT'], marker='^', label='ECLAT', linewidth=2)\n",
    "plt.title('Algorithm Execution Time Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Minimum Support', fontsize=12)\n",
    "plt.ylabel('Time (s)', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Export for Mobile App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export rules to JSON\n",
    "rules_list = []\n",
    "for _, row in rules.iterrows():\n",
    "    rule = {\n",
    "        'antecedents': list(row['antecedents']),\n",
    "        'consequents': list(row['consequents']),\n",
    "        'support': float(row['support']),\n",
    "        'confidence': float(row['confidence']),\n",
    "        'lift': float(row['lift'])\n",
    "    }\n",
    "    rules_list.append(rule)\n",
    "\n",
    "export_data = {\n",
    "    'metadata': {\n",
    "        'total_rules': len(rules),\n",
    "        'min_support': MIN_SUPPORT,\n",
    "        'min_confidence': MIN_CONFIDENCE,\n",
    "        'min_lift': MIN_LIFT,\n",
    "        'total_symptoms': len(symptom_cols)\n",
    "    },\n",
    "    'symptoms': sorted(symptom_cols),\n",
    "    'rules': rules_list\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('association_rules.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Exported {len(rules_list)} rules to association_rules.json\")\n",
    "print(f\"  Ready for Flutter mobile app integration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total patients: {len(df)}\")\n",
    "print(f\"  Total symptoms: {len(symptom_cols)}\")\n",
    "print(f\"  Avg symptoms per patient: {df_binary.sum(axis=1).mean():.2f}\")\n",
    "\n",
    "print(f\"\\nAssociation Mining:\")\n",
    "print(f\"  Frequent itemsets: {len(frequent_itemsets)}\")\n",
    "print(f\"  Association rules: {len(rules)}\")\n",
    "print(f\"  Avg confidence: {rules['confidence'].mean():.3f}\")\n",
    "print(f\"  Avg lift: {rules['lift'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\nTop Symptom Associations:\")\n",
    "for idx, row in rules.head(5).iterrows():\n",
    "    ant = ', '.join(list(row['antecedents']))\n",
    "    cons = ', '.join(list(row['consequents']))\n",
    "    print(f\"  {ant} → {cons} (lift: {row['lift']:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Analysis complete! Download association_rules.json for mobile app.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
